{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Avalia√ß√£o de Modelos - NeuroTranslator PT-EN\n",
    "\n",
    "Este notebook implementa m√©tricas de avalia√ß√£o para os modelos de tradu√ß√£o, incluindo BLEU, ROUGE, METEOR e an√°lises qualitativas.\n",
    "\n",
    "## M√©tricas Implementadas:\n",
    "- **BLEU**: Bilingual Evaluation Understudy\n",
    "- **ROUGE**: Recall-Oriented Understudy for Gisting Evaluation\n",
    "- **METEOR**: Metric for Evaluation of Translation with Explicit ORdering\n",
    "- **ChrF**: Character n-gram F-score\n",
    "- **An√°lise Qualitativa**: Exemplos e casos de erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√µes necess√°rias\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# M√©tricas de avalia√ß√£o\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "import sacrebleu\n",
    "\n",
    "# Processamento de texto\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Modelos\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Modelos e Dados de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados de teste\n",
    "def load_test_data():\n",
    "    \"\"\"\n",
    "    Carrega dados de teste para avalia√ß√£o\n",
    "    \"\"\"\n",
    "    # Dados de teste sint√©ticos (substituir por dados reais)\n",
    "    test_data = {\n",
    "        'portuguese': [\n",
    "            \"Ol√°, como voc√™ est√° hoje?\",\n",
    "            \"Eu gosto muito de programar em Python.\",\n",
    "            \"O tempo est√° muito bom para sair.\",\n",
    "            \"Vamos trabalhar juntos neste projeto interessante.\",\n",
    "            \"A intelig√™ncia artificial est√° revolucionando o mundo.\",\n",
    "            \"Preciso aprender mais sobre redes neurais profundas.\",\n",
    "            \"Este √© um sistema de tradu√ß√£o autom√°tica avan√ßado.\",\n",
    "            \"Vou estudar machine learning durante as f√©rias.\",\n",
    "            \"A tecnologia est√° evoluindo muito rapidamente.\",\n",
    "            \"Gosto de resolver problemas complexos de programa√ß√£o.\"\n",
    "        ],\n",
    "        'english_reference': [\n",
    "            \"Hello, how are you today?\",\n",
    "            \"I really like programming in Python.\",\n",
    "            \"The weather is very good for going out.\",\n",
    "            \"Let's work together on this interesting project.\",\n",
    "            \"Artificial intelligence is revolutionizing the world.\",\n",
    "            \"I need to learn more about deep neural networks.\",\n",
    "            \"This is an advanced automatic translation system.\",\n",
    "            \"I will study machine learning during the holidays.\",\n",
    "            \"Technology is evolving very rapidly.\",\n",
    "            \"I like solving complex programming problems.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(test_data)\n",
    "\n",
    "# Carregar dados de teste\n",
    "test_df = load_test_data()\n",
    "print(f\"üìä Dados de teste carregados: {len(test_df)} pares\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulador de tradu√ß√µes dos modelos (substituir por modelos reais)\n",
    "class ModelSimulator:\n",
    "    \"\"\"\n",
    "    Simula tradu√ß√µes de diferentes modelos para demonstra√ß√£o\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Tradu√ß√µes simuladas com diferentes qualidades\n",
    "        self.translations = {\n",
    "            \"Ol√°, como voc√™ est√° hoje?\": {\n",
    "                \"cnn_rnn\": \"Hello, how you are today?\",\n",
    "                \"transformer\": \"Hello, how are you today?\",\n",
    "                \"t5_finetuned\": \"Hello, how are you today?\"\n",
    "            },\n",
    "            \"Eu gosto muito de programar em Python.\": {\n",
    "                \"cnn_rnn\": \"I like very much programming in Python.\",\n",
    "                \"transformer\": \"I really like programming in Python.\",\n",
    "                \"t5_finetuned\": \"I really like to program in Python.\"\n",
    "            },\n",
    "            \"O tempo est√° muito bom para sair.\": {\n",
    "                \"cnn_rnn\": \"The time is very good for go out.\",\n",
    "                \"transformer\": \"The weather is very good for going out.\",\n",
    "                \"t5_finetuned\": \"The weather is very nice to go out.\"\n",
    "            },\n",
    "            \"Vamos trabalhar juntos neste projeto interessante.\": {\n",
    "                \"cnn_rnn\": \"Let's work together in this interesting project.\",\n",
    "                \"transformer\": \"Let's work together on this interesting project.\",\n",
    "                \"t5_finetuned\": \"Let's work together on this interesting project.\"\n",
    "            },\n",
    "            \"A intelig√™ncia artificial est√° revolucionando o mundo.\": {\n",
    "                \"cnn_rnn\": \"The artificial intelligence is revolutionizing world.\",\n",
    "                \"transformer\": \"Artificial intelligence is revolutionizing the world.\",\n",
    "                \"t5_finetuned\": \"Artificial intelligence is revolutionizing the world.\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def translate(self, text):\n",
    "        if text in self.translations:\n",
    "            return self.translations[text].get(self.model_name, \"[Translation not available]\")\n",
    "        else:\n",
    "            # Tradu√ß√£o gen√©rica para textos n√£o mapeados\n",
    "            return f\"[{self.model_name} translation of: {text}]\"\n",
    "\n",
    "# Criar simuladores dos modelos\n",
    "models = {\n",
    "    'cnn_rnn': ModelSimulator('cnn_rnn'),\n",
    "    'transformer': ModelSimulator('transformer'),\n",
    "    't5_finetuned': ModelSimulator('t5_finetuned')\n",
    "}\n",
    "\n",
    "print(\"ü§ñ Simuladores de modelos criados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementa√ß√£o das M√©tricas de Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationEvaluator:\n",
    "    \"\"\"\n",
    "    Classe para avaliar qualidade de tradu√ß√µes\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.smoothing = SmoothingFunction().method1\n",
    "        \n",
    "        # Download recursos NLTK se necess√°rio\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "            \n",
    "        try:\n",
    "            nltk.data.find('corpora/wordnet')\n",
    "        except LookupError:\n",
    "            nltk.download('wordnet')\n",
    "    \n",
    "    def calculate_bleu(self, reference, hypothesis):\n",
    "        \"\"\"\n",
    "        Calcula BLEU score\n",
    "        \"\"\"\n",
    "        reference_tokens = [word_tokenize(reference.lower())]\n",
    "        hypothesis_tokens = word_tokenize(hypothesis.lower())\n",
    "        \n",
    "        # BLEU-1, BLEU-2, BLEU-3, BLEU-4\n",
    "        bleu_scores = {}\n",
    "        for n in range(1, 5):\n",
    "            weights = [1/n] * n + [0] * (4-n)\n",
    "            bleu_scores[f'bleu_{n}'] = sentence_bleu(\n",
    "                reference_tokens, hypothesis_tokens, \n",
    "                weights=weights, smoothing_function=self.smoothing\n",
    "            )\n",
    "        \n",
    "        return bleu_scores\n",
    "    \n",
    "    def calculate_rouge(self, reference, hypothesis):\n",
    "        \"\"\"\n",
    "        Calcula ROUGE scores\n",
    "        \"\"\"\n",
    "        scores = self.rouge_scorer.score(reference, hypothesis)\n",
    "        \n",
    "        rouge_scores = {}\n",
    "        for metric, score in scores.items():\n",
    "            rouge_scores[f'{metric}_precision'] = score.precision\n",
    "            rouge_scores[f'{metric}_recall'] = score.recall\n",
    "            rouge_scores[f'{metric}_f1'] = score.fmeasure\n",
    "        \n",
    "        return rouge_scores\n",
    "    \n",
    "    def calculate_meteor(self, reference, hypothesis):\n",
    "        \"\"\"\n",
    "        Calcula METEOR score\n",
    "        \"\"\"\n",
    "        try:\n",
    "            score = meteor_score([word_tokenize(reference)], word_tokenize(hypothesis))\n",
    "            return {'meteor': score}\n",
    "        except:\n",
    "            return {'meteor': 0.0}\n",
    "    \n",
    "    def calculate_chrf(self, reference, hypothesis):\n",
    "        \"\"\"\n",
    "        Calcula ChrF score (Character n-gram F-score)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            score = sacrebleu.sentence_chrf(hypothesis, [reference])\n",
    "            return {'chrf': score.score / 100.0}  # Normalizar para 0-1\n",
    "        except:\n",
    "            # Implementa√ß√£o simplificada se sacrebleu n√£o estiver dispon√≠vel\n",
    "            ref_chars = set(reference.lower().replace(' ', ''))\n",
    "            hyp_chars = set(hypothesis.lower().replace(' ', ''))\n",
    "            \n",
    "            if len(hyp_chars) == 0:\n",
    "                return {'chrf': 0.0}\n",
    "            \n",
    "            precision = len(ref_chars & hyp_chars) / len(hyp_chars)\n",
    "            recall = len(ref_chars & hyp_chars) / len(ref_chars) if len(ref_chars) > 0 else 0\n",
    "            \n",
    "            if precision + recall == 0:\n",
    "                f1 = 0.0\n",
    "            else:\n",
    "                f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            \n",
    "            return {'chrf': f1}\n",
    "    \n",
    "    def evaluate_translation(self, reference, hypothesis):\n",
    "        \"\"\"\n",
    "        Avalia uma tradu√ß√£o com todas as m√©tricas\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # BLEU scores\n",
    "        metrics.update(self.calculate_bleu(reference, hypothesis))\n",
    "        \n",
    "        # ROUGE scores\n",
    "        metrics.update(self.calculate_rouge(reference, hypothesis))\n",
    "        \n",
    "        # METEOR score\n",
    "        metrics.update(self.calculate_meteor(reference, hypothesis))\n",
    "        \n",
    "        # ChrF score\n",
    "        metrics.update(self.calculate_chrf(reference, hypothesis))\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Criar avaliador\n",
    "evaluator = TranslationEvaluator()\n",
    "print(\"üìè Avaliador de tradu√ß√µes criado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Avalia√ß√£o dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar tradu√ß√µes e avaliar\n",
    "def evaluate_models(test_df, models, evaluator):\n",
    "    \"\"\"\n",
    "    Avalia todos os modelos no conjunto de teste\n",
    "    \"\"\"\n",
    "    results = defaultdict(list)\n",
    "    detailed_results = []\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        source = row['portuguese']\n",
    "        reference = row['english_reference']\n",
    "        \n",
    "        print(f\"\\nüìù Avaliando: '{source}'\")\n",
    "        print(f\"üéØ Refer√™ncia: '{reference}'\")\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            # Gerar tradu√ß√£o\n",
    "            hypothesis = model.translate(source)\n",
    "            print(f\"ü§ñ {model_name}: '{hypothesis}'\")\n",
    "            \n",
    "            # Avaliar tradu√ß√£o\n",
    "            metrics = evaluator.evaluate_translation(reference, hypothesis)\n",
    "            \n",
    "            # Armazenar resultados\n",
    "            result_entry = {\n",
    "                'model': model_name,\n",
    "                'source': source,\n",
    "                'reference': reference,\n",
    "                'hypothesis': hypothesis,\n",
    "                **metrics\n",
    "            }\n",
    "            detailed_results.append(result_entry)\n",
    "            \n",
    "            # Adicionar √†s m√©dias\n",
    "            for metric, value in metrics.items():\n",
    "                results[f'{model_name}_{metric}'].append(value)\n",
    "    \n",
    "    return results, detailed_results\n",
    "\n",
    "# Executar avalia√ß√£o\n",
    "print(\"üöÄ Iniciando avalia√ß√£o dos modelos...\")\n",
    "results, detailed_results = evaluate_models(test_df[:5], models, evaluator)  # Usar apenas 5 exemplos para demonstra√ß√£o\n",
    "\n",
    "print(\"\\n‚úÖ Avalia√ß√£o conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An√°lise dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular m√©dias das m√©tricas\n",
    "def calculate_average_metrics(results):\n",
    "    \"\"\"\n",
    "    Calcula m√©dias das m√©tricas por modelo\n",
    "    \"\"\"\n",
    "    avg_metrics = {}\n",
    "    \n",
    "    for key, values in results.items():\n",
    "        if values:  # Verificar se a lista n√£o est√° vazia\n",
    "            avg_metrics[key] = np.mean(values)\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "# Calcular m√©dias\n",
    "avg_metrics = calculate_average_metrics(results)\n",
    "\n",
    "# Organizar resultados por modelo\n",
    "model_metrics = defaultdict(dict)\n",
    "for key, value in avg_metrics.items():\n",
    "    model_name, metric_name = key.split('_', 1)\n",
    "    model_metrics[model_name][metric_name] = value\n",
    "\n",
    "# Criar DataFrame para visualiza√ß√£o\n",
    "metrics_df = pd.DataFrame(model_metrics).T\n",
    "print(\"üìä M√©tricas m√©dias por modelo:\")\n",
    "print(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar principais m√©tricas\n",
    "main_metrics = ['bleu_4', 'rouge1_f1', 'rougeL_f1', 'meteor', 'chrf']\n",
    "available_metrics = [m for m in main_metrics if m in metrics_df.columns]\n",
    "\n",
    "if available_metrics:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, metric in enumerate(available_metrics):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Gr√°fico de barras\n",
    "            models_list = list(metrics_df.index)\n",
    "            values = [metrics_df.loc[model, metric] for model in models_list]\n",
    "            \n",
    "            bars = ax.bar(models_list, values, alpha=0.8)\n",
    "            ax.set_title(f'{metric.upper()} Score', fontsize=14, fontweight='bold')\n",
    "            ax.set_ylabel('Score', fontsize=12)\n",
    "            ax.set_ylim(0, 1)\n",
    "            \n",
    "            # Adicionar valores nas barras\n",
    "            for bar, value in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remover subplots vazios\n",
    "    for i in range(len(available_metrics), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è M√©tricas n√£o dispon√≠veis para visualiza√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An√°lise Qualitativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise qualitativa detalhada\n",
    "def analyze_translation_quality(detailed_results):\n",
    "    \"\"\"\n",
    "    An√°lise qualitativa das tradu√ß√µes\n",
    "    \"\"\"\n",
    "    print(\"üîç AN√ÅLISE QUALITATIVA DAS TRADU√á√ïES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Agrupar por fonte\n",
    "    by_source = defaultdict(list)\n",
    "    for result in detailed_results:\n",
    "        by_source[result['source']].append(result)\n",
    "    \n",
    "    for source, translations in by_source.items():\n",
    "        print(f\"\\nüìù ORIGINAL: {source}\")\n",
    "        \n",
    "        # Mostrar refer√™ncia\n",
    "        reference = translations[0]['reference']\n",
    "        print(f\"üéØ REFER√äNCIA: {reference}\")\n",
    "        \n",
    "        print(\"\\nü§ñ TRADU√á√ïES DOS MODELOS:\")\n",
    "        \n",
    "        # Ordenar por BLEU-4 score\n",
    "        translations.sort(key=lambda x: x.get('bleu_4', 0), reverse=True)\n",
    "        \n",
    "        for i, trans in enumerate(translations, 1):\n",
    "            model = trans['model']\n",
    "            hypothesis = trans['hypothesis']\n",
    "            bleu4 = trans.get('bleu_4', 0)\n",
    "            rouge1_f1 = trans.get('rouge1_f1', 0)\n",
    "            \n",
    "            print(f\"  {i}. {model.upper()}: {hypothesis}\")\n",
    "            print(f\"     üìä BLEU-4: {bleu4:.3f} | ROUGE-1 F1: {rouge1_f1:.3f}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Executar an√°lise qualitativa\n",
    "analyze_translation_quality(detailed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. An√°lise de Erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(detailed_results):\n",
    "    \"\"\"\n",
    "    An√°lise de tipos de erros comuns\n",
    "    \"\"\"\n",
    "    print(\"üîç AN√ÅLISE DE ERROS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    error_patterns = {\n",
    "        'word_order': [],\n",
    "        'missing_words': [],\n",
    "        'wrong_prepositions': [],\n",
    "        'grammatical_errors': [],\n",
    "        'lexical_errors': []\n",
    "    }\n",
    "    \n",
    "    # An√°lise simplificada de erros\n",
    "    for result in detailed_results:\n",
    "        reference = result['reference'].lower()\n",
    "        hypothesis = result['hypothesis'].lower()\n",
    "        model = result['model']\n",
    "        \n",
    "        # Detectar alguns padr√µes de erro simples\n",
    "        ref_words = set(word_tokenize(reference))\n",
    "        hyp_words = set(word_tokenize(hypothesis))\n",
    "        \n",
    "        # Palavras ausentes\n",
    "        missing = ref_words - hyp_words\n",
    "        if missing:\n",
    "            error_patterns['missing_words'].append({\n",
    "                'model': model,\n",
    "                'missing': list(missing),\n",
    "                'sentence': result['source']\n",
    "            })\n",
    "        \n",
    "        # Erros de preposi√ß√£o (exemplos espec√≠ficos)\n",
    "        if 'in this' in reference and 'in this' not in hypothesis:\n",
    "            if 'on this' in hypothesis:\n",
    "                error_patterns['wrong_prepositions'].append({\n",
    "                    'model': model,\n",
    "                    'error': 'in -> on',\n",
    "                    'sentence': result['source']\n",
    "                })\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    for error_type, errors in error_patterns.items():\n",
    "        if errors:\n",
    "            print(f\"\\nüìã {error_type.replace('_', ' ').title()}:\")\n",
    "            for error in errors[:3]:  # Mostrar apenas os primeiros 3\n",
    "                print(f\"  ‚Ä¢ Modelo: {error['model']}\")\n",
    "                if 'missing' in error:\n",
    "                    print(f\"    Palavras ausentes: {error['missing']}\")\n",
    "                elif 'error' in error:\n",
    "                    print(f\"    Erro: {error['error']}\")\n",
    "                print(f\"    Frase: {error['sentence']}\")\n",
    "                print()\n",
    "\n",
    "# Executar an√°lise de erros\n",
    "error_analysis(detailed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compara√ß√£o de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking dos modelos\n",
    "def rank_models(metrics_df):\n",
    "    \"\"\"\n",
    "    Cria ranking dos modelos baseado nas m√©tricas\n",
    "    \"\"\"\n",
    "    # M√©tricas principais para ranking\n",
    "    key_metrics = ['bleu_4', 'rouge1_f1', 'meteor', 'chrf']\n",
    "    available_key_metrics = [m for m in key_metrics if m in metrics_df.columns]\n",
    "    \n",
    "    if not available_key_metrics:\n",
    "        print(\"‚ö†Ô∏è M√©tricas principais n√£o dispon√≠veis para ranking\")\n",
    "        return\n",
    "    \n",
    "    # Calcular score composto (m√©dia das m√©tricas normalizadas)\n",
    "    composite_scores = {}\n",
    "    \n",
    "    for model in metrics_df.index:\n",
    "        scores = []\n",
    "        for metric in available_key_metrics:\n",
    "            score = metrics_df.loc[model, metric]\n",
    "            scores.append(score)\n",
    "        \n",
    "        composite_scores[model] = np.mean(scores)\n",
    "    \n",
    "    # Ordenar por score composto\n",
    "    ranked_models = sorted(composite_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"üèÜ RANKING DOS MODELOS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    for i, (model, score) in enumerate(ranked_models, 1):\n",
    "        medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else \"üìä\"\n",
    "        print(f\"{medal} {i}¬∫ lugar: {model.upper()}\")\n",
    "        print(f\"   Score Composto: {score:.4f}\")\n",
    "        \n",
    "        # Mostrar m√©tricas individuais\n",
    "        for metric in available_key_metrics:\n",
    "            value = metrics_df.loc[model, metric]\n",
    "            print(f\"   {metric}: {value:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    return ranked_models\n",
    "\n",
    "# Executar ranking\n",
    "ranking = rank_models(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Salvamento dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados da avalia√ß√£o\n",
    "import os\n",
    "\n",
    "results_dir = '../docs/evaluation_results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Salvar m√©tricas m√©dias\n",
    "metrics_df.to_csv(f'{results_dir}/average_metrics.csv')\n",
    "\n",
    "# Salvar resultados detalhados\n",
    "detailed_df = pd.DataFrame(detailed_results)\n",
    "detailed_df.to_csv(f'{results_dir}/detailed_results.csv', index=False)\n",
    "\n",
    "# Salvar ranking\n",
    "if ranking:\n",
    "    ranking_df = pd.DataFrame(ranking, columns=['model', 'composite_score'])\n",
    "    ranking_df.to_csv(f'{results_dir}/model_ranking.csv', index=False)\n",
    "\n",
    "# Salvar relat√≥rio em JSON\n",
    "evaluation_report = {\n",
    "    'evaluation_date': datetime.now().isoformat(),\n",
    "    'test_samples': len(test_df),\n",
    "    'models_evaluated': list(models.keys()),\n",
    "    'metrics_used': list(available_metrics) if 'available_metrics' in locals() else [],\n",
    "    'average_metrics': metrics_df.to_dict(),\n",
    "    'ranking': ranking if ranking else []\n",
    "}\n",
    "\n",
    "with open(f'{results_dir}/evaluation_report.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(evaluation_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"üíæ Resultados salvos em: {results_dir}/\")\n",
    "print(\"üìÅ Arquivos gerados:\")\n",
    "print(\"  ‚Ä¢ average_metrics.csv - M√©tricas m√©dias por modelo\")\n",
    "print(\"  ‚Ä¢ detailed_results.csv - Resultados detalhados\")\n",
    "print(\"  ‚Ä¢ model_ranking.csv - Ranking dos modelos\")\n",
    "print(\"  ‚Ä¢ evaluation_report.json - Relat√≥rio completo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recomenda√ß√µes e Pr√≥ximos Passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar recomenda√ß√µes baseadas nos resultados\n",
    "def generate_recommendations(metrics_df, ranking):\n",
    "    \"\"\"\n",
    "    Gera recomenda√ß√µes baseadas na avalia√ß√£o\n",
    "    \"\"\"\n",
    "    print(\"üí° RECOMENDA√á√ïES E PR√ìXIMOS PASSOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if ranking:\n",
    "        best_model = ranking[0][0]\n",
    "        best_score = ranking[0][1]\n",
    "        \n",
    "        print(f\"\\nüéØ Melhor Modelo: {best_model.upper()}\")\n",
    "        print(f\"   Score Composto: {best_score:.4f}\")\n",
    "        \n",
    "        if best_score < 0.5:\n",
    "            print(\"\\n‚ö†Ô∏è ATEN√á√ÉO: Scores baixos detectados\")\n",
    "            print(\"üìã Recomenda√ß√µes:\")\n",
    "            print(\"  1. Aumentar dataset de treinamento\")\n",
    "            print(\"  2. Implementar data augmentation\")\n",
    "            print(\"  3. Ajustar hiperpar√¢metros\")\n",
    "            print(\"  4. Considerar modelos pr√©-treinados maiores\")\n",
    "        \n",
    "        elif best_score < 0.7:\n",
    "            print(\"\\nüìà Performance moderada\")\n",
    "            print(\"üìã Sugest√µes de melhoria:\")\n",
    "            print(\"  1. Fine-tuning mais espec√≠fico\")\n",
    "            print(\"  2. Ensemble de modelos\")\n",
    "            print(\"  3. P√≥s-processamento das tradu√ß√µes\")\n",
    "        \n",
    "        else:\n",
    "            print(\"\\n‚úÖ Boa performance!\")\n",
    "            print(\"üìã Pr√≥ximos passos:\")\n",
    "            print(\"  1. Testes com usu√°rios reais\")\n",
    "            print(\"  2. Otimiza√ß√£o para produ√ß√£o\")\n",
    "            print(\"  3. Implementa√ß√£o de cache\")\n",
    "    \n",
    "    print(\"\\nüîß Melhorias T√©cnicas Sugeridas:\")\n",
    "    print(\"  ‚Ä¢ Implementar beam search para decodifica√ß√£o\")\n",
    "    print(\"  ‚Ä¢ Adicionar attention visualization\")\n",
    "    print(\"  ‚Ä¢ Implementar back-translation para augmenta√ß√£o\")\n",
    "    print(\"  ‚Ä¢ Criar sistema de feedback do usu√°rio\")\n",
    "    \n",
    "    print(\"\\nüìä M√©tricas Adicionais para Implementar:\")\n",
    "    print(\"  ‚Ä¢ BERTScore para similaridade sem√¢ntica\")\n",
    "    print(\"  ‚Ä¢ Avalia√ß√£o humana (adequacy & fluency)\")\n",
    "    print(\"  ‚Ä¢ Tempo de infer√™ncia\")\n",
    "    print(\"  ‚Ä¢ Uso de mem√≥ria\")\n",
    "\n",
    "# Gerar recomenda√ß√µes\n",
    "generate_recommendations(metrics_df, ranking if 'ranking' in locals() else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Resumo da Avalia√ß√£o\n",
    "\n",
    "### M√©tricas Implementadas:\n",
    "- **BLEU (1-4)**: Precis√£o de n-gramas\n",
    "- **ROUGE (1, 2, L)**: Recall de n-gramas e sequ√™ncias longas\n",
    "- **METEOR**: Alinhamento com sin√¥nimos\n",
    "- **ChrF**: F-score baseado em caracteres\n",
    "\n",
    "### An√°lises Realizadas:\n",
    "1. **Quantitativa**: Scores autom√°ticos padronizados\n",
    "2. **Qualitativa**: Compara√ß√£o manual das tradu√ß√µes\n",
    "3. **An√°lise de Erros**: Identifica√ß√£o de padr√µes problem√°ticos\n",
    "4. **Ranking**: Classifica√ß√£o dos modelos por performance\n",
    "\n",
    "### Pr√≥ximas Implementa√ß√µes:\n",
    "1. **Avalia√ß√£o Humana**: Adequacy e Fluency scores\n",
    "2. **M√©tricas Neurais**: BERTScore, BLEURT\n",
    "3. **An√°lise de Dom√≠nio**: Performance por tipo de texto\n",
    "4. **Testes A/B**: Compara√ß√£o com usu√°rios reais\n",
    "\n",
    "---\n",
    "\n",
    "**Desenvolvido para o NeuroTranslator PT-EN** üß†üîÑüåê"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}