{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Explora√ß√£o de Dados - NeuroTranslator PT-EN\n",
    "\n",
    "Este notebook cont√©m a an√°lise explorat√≥ria dos datasets de tradu√ß√£o Portugu√™s-Ingl√™s que ser√£o utilizados para treinar nossos modelos neurais.\n",
    "\n",
    "## Objetivos:\n",
    "- Carregar e analisar datasets paralelos PT-EN\n",
    "- Estat√≠sticas descritivas dos textos\n",
    "- An√°lise de distribui√ß√£o de comprimento\n",
    "- Identifica√ß√£o de padr√µes lingu√≠sticos\n",
    "- Prepara√ß√£o para pr√©-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√µes necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Datasets\n",
    "\n",
    "Vamos carregar datasets p√∫blicos de tradu√ß√£o PT-EN dispon√≠veis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download de recursos NLTK necess√°rios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Carregamento de modelos spaCy\n",
    "try:\n",
    "    nlp_pt = spacy.load('pt_core_news_sm')\n",
    "    nlp_en = spacy.load('en_core_web_sm')\n",
    "    print(\"‚úÖ Modelos spaCy carregados com sucesso!\")\n",
    "except OSError:\n",
    "    print(\"‚ö†Ô∏è Modelos spaCy n√£o encontrados. Execute:\")\n",
    "    print(\"python -m spacy download pt_core_news_sm\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para carregar datasets de tradu√ß√£o\n",
    "def load_translation_datasets():\n",
    "    \"\"\"\n",
    "    Carrega datasets de tradu√ß√£o PT-EN de fontes p√∫blicas\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # Dataset 1: OpenSubtitles (exemplo)\n",
    "    # Aqui voc√™ pode adicionar c√≥digo para carregar datasets reais\n",
    "    \n",
    "    # Dataset sint√©tico para demonstra√ß√£o\n",
    "    sample_data = {\n",
    "        'portuguese': [\n",
    "            \"Ol√°, como voc√™ est√°?\",\n",
    "            \"Eu gosto de programar em Python.\",\n",
    "            \"O tempo est√° muito bom hoje.\",\n",
    "            \"Vamos trabalhar juntos neste projeto.\",\n",
    "            \"A intelig√™ncia artificial √© fascinante.\"\n",
    "        ],\n",
    "        'english': [\n",
    "            \"Hello, how are you?\",\n",
    "            \"I like programming in Python.\",\n",
    "            \"The weather is very nice today.\",\n",
    "            \"Let's work together on this project.\",\n",
    "            \"Artificial intelligence is fascinating.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    datasets['sample'] = pd.DataFrame(sample_data)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Carregar datasets\n",
    "datasets = load_translation_datasets()\n",
    "print(f\"üìä Datasets carregados: {list(datasets.keys())}\")\n",
    "\n",
    "# Visualizar amostra\n",
    "sample_df = datasets['sample']\n",
    "print(f\"\\nüìà Tamanho do dataset de exemplo: {len(sample_df)} pares\")\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. An√°lise Estat√≠stica B√°sica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_statistics(df):\n",
    "    \"\"\"\n",
    "    Analisa estat√≠sticas b√°sicas dos textos\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    for lang in ['portuguese', 'english']:\n",
    "        texts = df[lang].astype(str)\n",
    "        \n",
    "        # Estat√≠sticas de comprimento\n",
    "        char_lengths = texts.str.len()\n",
    "        word_counts = texts.str.split().str.len()\n",
    "        \n",
    "        stats[lang] = {\n",
    "            'total_sentences': len(texts),\n",
    "            'avg_char_length': char_lengths.mean(),\n",
    "            'avg_word_count': word_counts.mean(),\n",
    "            'max_char_length': char_lengths.max(),\n",
    "            'min_char_length': char_lengths.min(),\n",
    "            'std_char_length': char_lengths.std(),\n",
    "            'std_word_count': word_counts.std()\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analisar estat√≠sticas\n",
    "stats = analyze_text_statistics(sample_df)\n",
    "\n",
    "# Criar DataFrame para visualiza√ß√£o\n",
    "stats_df = pd.DataFrame(stats).T\n",
    "print(\"üìä Estat√≠sticas dos Textos:\")\n",
    "stats_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualiza√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de distribui√ß√£o de comprimento\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Comprimento em caracteres\n",
    "pt_chars = sample_df['portuguese'].str.len()\n",
    "en_chars = sample_df['english'].str.len()\n",
    "\n",
    "axes[0, 0].hist(pt_chars, bins=20, alpha=0.7, label='Portugu√™s', color='blue')\n",
    "axes[0, 0].set_title('Distribui√ß√£o - Caracteres (PT)')\n",
    "axes[0, 0].set_xlabel('N√∫mero de Caracteres')\n",
    "axes[0, 0].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "axes[0, 1].hist(en_chars, bins=20, alpha=0.7, label='Ingl√™s', color='red')\n",
    "axes[0, 1].set_title('Distribui√ß√£o - Caracteres (EN)')\n",
    "axes[0, 1].set_xlabel('N√∫mero de Caracteres')\n",
    "axes[0, 1].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "# Comprimento em palavras\n",
    "pt_words = sample_df['portuguese'].str.split().str.len()\n",
    "en_words = sample_df['english'].str.split().str.len()\n",
    "\n",
    "axes[1, 0].hist(pt_words, bins=20, alpha=0.7, label='Portugu√™s', color='blue')\n",
    "axes[1, 0].set_title('Distribui√ß√£o - Palavras (PT)')\n",
    "axes[1, 0].set_xlabel('N√∫mero de Palavras')\n",
    "axes[1, 0].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "axes[1, 1].hist(en_words, bins=20, alpha=0.7, label='Ingl√™s', color='red')\n",
    "axes[1, 1].set_title('Distribui√ß√£o - Palavras (EN)')\n",
    "axes[1, 1].set_xlabel('N√∫mero de Palavras')\n",
    "axes[1, 1].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An√°lise de Vocabul√°rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_vocabulary(texts, language='portuguese'):\n",
    "    \"\"\"\n",
    "    Analisa o vocabul√°rio dos textos\n",
    "    \"\"\"\n",
    "    # Tokeniza√ß√£o\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = word_tokenize(text.lower())\n",
    "        all_words.extend([word for word in words if word.isalpha()])\n",
    "    \n",
    "    # Estat√≠sticas de vocabul√°rio\n",
    "    vocab_stats = {\n",
    "        'total_words': len(all_words),\n",
    "        'unique_words': len(set(all_words)),\n",
    "        'vocabulary_richness': len(set(all_words)) / len(all_words) if all_words else 0\n",
    "    }\n",
    "    \n",
    "    # Palavras mais frequentes\n",
    "    word_freq = Counter(all_words)\n",
    "    most_common = word_freq.most_common(10)\n",
    "    \n",
    "    return vocab_stats, most_common, all_words\n",
    "\n",
    "# Analisar vocabul√°rio portugu√™s\n",
    "pt_vocab_stats, pt_common, pt_words = analyze_vocabulary(sample_df['portuguese'], 'portuguese')\n",
    "en_vocab_stats, en_common, en_words = analyze_vocabulary(sample_df['english'], 'english')\n",
    "\n",
    "print(\"üáßüá∑ Estat√≠sticas do Vocabul√°rio Portugu√™s:\")\n",
    "for key, value in pt_vocab_stats.items():\n",
    "    print(f\"  {key}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nüá∫üá∏ Estat√≠sticas do Vocabul√°rio Ingl√™s:\")\n",
    "for key, value in en_vocab_stats.items():\n",
    "    print(f\"  {key}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nüìù Palavras mais comuns (PT):\", pt_common[:5])\n",
    "print(\"üìù Palavras mais comuns (EN):\", en_common[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepara√ß√£o para Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o de pr√©-processamento b√°sico\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Pr√©-processamento b√°sico do texto\n",
    "    \"\"\"\n",
    "    # Converter para min√∫sculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover caracteres especiais (manter pontua√ß√£o b√°sica)\n",
    "    import re\n",
    "    text = re.sub(r'[^a-z√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ß\\s.,!?]', '', text)\n",
    "    \n",
    "    # Normalizar espa√ßos\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Aplicar pr√©-processamento\n",
    "sample_df['portuguese_clean'] = sample_df['portuguese'].apply(preprocess_text)\n",
    "sample_df['english_clean'] = sample_df['english'].apply(preprocess_text)\n",
    "\n",
    "print(\"üßπ Exemplo de pr√©-processamento:\")\n",
    "print(\"Original (PT):\", sample_df['portuguese'].iloc[0])\n",
    "print(\"Limpo (PT):   \", sample_df['portuguese_clean'].iloc[0])\n",
    "print(\"\\nOriginal (EN):\", sample_df['english'].iloc[0])\n",
    "print(\"Limpo (EN):   \", sample_df['english_clean'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Salvamento dos Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar dados processados\n",
    "output_path = '../data/processed_translation_data.csv'\n",
    "sample_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"üíæ Dados processados salvos em: {output_path}\")\n",
    "print(f\"üìä Total de pares de tradu√ß√£o: {len(sample_df)}\")\n",
    "\n",
    "# Resumo final\n",
    "print(\"\\nüìã Resumo da Explora√ß√£o:\")\n",
    "print(f\"  ‚Ä¢ Dataset analisado com {len(sample_df)} pares PT-EN\")\n",
    "print(f\"  ‚Ä¢ Comprimento m√©dio (PT): {sample_df['portuguese'].str.len().mean():.1f} caracteres\")\n",
    "print(f\"  ‚Ä¢ Comprimento m√©dio (EN): {sample_df['english'].str.len().mean():.1f} caracteres\")\n",
    "print(f\"  ‚Ä¢ Vocabul√°rio √∫nico (PT): {len(set(' '.join(sample_df['portuguese']).split()))} palavras\")\n",
    "print(f\"  ‚Ä¢ Vocabul√°rio √∫nico (EN): {len(set(' '.join(sample_df['english']).split()))} palavras\")\n",
    "print(\"\\n‚úÖ Explora√ß√£o de dados conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Pr√≥ximos Passos\n",
    "\n",
    "1. **Coleta de Dados Reais**: Integrar datasets maiores como OpenSubtitles, OPUS, etc.\n",
    "2. **An√°lise Lingu√≠stica Avan√ßada**: POS tagging, an√°lise sint√°tica\n",
    "3. **Prepara√ß√£o para Treinamento**: Tokeniza√ß√£o, cria√ß√£o de vocabul√°rios\n",
    "4. **Augmenta√ß√£o de Dados**: T√©cnicas para aumentar o dataset\n",
    "5. **Valida√ß√£o Cruzada**: Divis√£o em treino/valida√ß√£o/teste\n",
    "\n",
    "---\n",
    "\n",
    "**Desenvolvido para o NeuroTranslator PT-EN** üß†üîÑüåê"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroTranslator (Python 3.11)",
   "language": "python",
   "name": "neurotranslator_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
